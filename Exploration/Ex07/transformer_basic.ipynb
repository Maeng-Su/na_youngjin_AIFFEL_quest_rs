{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 592,
   "id": "9ca88220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "print(tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e934f037",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "id": "e6d2da74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12시 땡!</td>\n",
       "      <td>하루가 또 가네요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1지망 학교 떨어졌어</td>\n",
       "      <td>위로해 드립니다.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3박4일 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3박4일 정도 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PPL 심하네</td>\n",
       "      <td>눈살이 찌푸려지죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Q            A  label\n",
       "0           12시 땡!   하루가 또 가네요.      0\n",
       "1      1지망 학교 떨어졌어    위로해 드립니다.      0\n",
       "2     3박4일 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
       "3  3박4일 정도 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
       "4          PPL 심하네   눈살이 찌푸려지죠.      0"
      ]
     },
     "execution_count": 593,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = '/aiffel/aiffel/transformer_chatbot/data/ChatbotData .csv'\n",
    "df = pd.read_csv(data_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "id": "807b0542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: 11823\n",
      "Q        0\n",
      "A        0\n",
      "label    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('data:', len(df))\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "id": "ccb5969a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(data,column=None):\n",
    "    res = []\n",
    "    if column:\n",
    "        for sentence in data[column]:\n",
    "            sentence =  sentence.lower().strip() # 소문자로 변경후 양쪽 공백 제거\n",
    "            sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence) # 특수문자와 분리\n",
    "            sentence = re.sub(r'[\" \"]+', \" \", sentence) # 공백 한칸으로 조정\n",
    "            sentence = re.sub(r\"[^^가-힣a-zA-Z0-9\\s.?!,]+\", \" \", sentence) #필요없는 문자들은 ' '로 대체, Korean\n",
    "            sentence = sentence.strip()\n",
    "            res.append(sentence)\n",
    "    else:\n",
    "        sentence =  data.lower().strip() \n",
    "        sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence) \n",
    "        sentence = re.sub(r'[\" \"]+', \" \", sentence) \n",
    "        sentence = re.sub(r\"[^^가-힣a-zA-Z0-9\\s.?!,]+\", \" \", sentence) \n",
    "        res = sentence.strip()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "id": "e379eb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = preprocess_sentence(df, 'Q')\n",
    "answers = preprocess_sentence(df, 'A')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "id": "5f6a163c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11823 11823\n"
     ]
    }
   ],
   "source": [
    "print(len(questions), len(answers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "id": "b7d05c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SubwordTextEncoder Vocabulary 생성\n",
    "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
    "    questions + answers, target_vocab_size=2**13)\n",
    "\n",
    "START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]\n",
    "VOCAB_SIZE = tokenizer.vocab_size + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "id": "1ab85c0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8166"
      ]
     },
     "execution_count": 599,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "id": "df847d5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정수 인코딩 전의 21번째 질문 샘플: 가스비 장난 아님\n",
      "정수 인코딩 후의 21번째 질문 샘플: [5756, 610, 2487, 4158]\n",
      "정수 디코딩 후의 21번째 질문 샘플: 가스비 장난 아님\n"
     ]
    }
   ],
   "source": [
    "print('정수 인코딩 전의 21번째 질문 샘플: {}'.format(questions[21]))\n",
    "print('정수 인코딩 후의 21번째 질문 샘플: {}'.format(tokenizer.encode(questions[21])))\n",
    "print('정수 디코딩 후의 21번째 질문 샘플: {}'.format(tokenizer.decode(tokenizer.encode(questions[21]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "id": "c04a12c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56 76\n"
     ]
    }
   ],
   "source": [
    "df['Qlength'] = df['Q'].apply(len)\n",
    "df['Alength'] = df['A'].apply(len)\n",
    "print(max(df['Qlength']), max(df['Alength']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "id": "1893b3f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(8,4))\n",
    "\n",
    "ax[0].hist(df['Qlength'], bins = 10, label='Q')\n",
    "ax[0].set_xlabel('length of Q ')\n",
    "ax[0].set_ylabel('number of Q ')\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].hist(df['Alength'], bins = 10, label='A')\n",
    "ax[1].set_xlabel('length of A ')\n",
    "ax[1].set_ylabel('number of A ')\n",
    "ax[1].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "id": "9744dc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 50 # 설정 기준 ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "id": "d320e091",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['12시 땡 !',\n",
       " '1지망 학교 떨어졌어',\n",
       " '3박4일 놀러가고 싶다',\n",
       " '3박4일 정도 놀러가고 싶다',\n",
       " 'ppl 심하네',\n",
       " 'sd카드 망가졌어',\n",
       " 'sd카드 안돼',\n",
       " 'sns 맞팔 왜 안하지',\n",
       " 'sns 시간낭비인 거 아는데 매일 하는 중',\n",
       " 'sns 시간낭비인데 자꾸 보게됨',\n",
       " 'sns보면 나만 빼고 다 행복해보여',\n",
       " '가끔 궁금해',\n",
       " '가끔 뭐하는지 궁금해',\n",
       " '가끔은 혼자인게 좋다',\n",
       " '가난한 자의 설움',\n",
       " '가만 있어도 땀난다',\n",
       " '가상화폐 쫄딱 망함',\n",
       " '가스불 켜고 나갔어',\n",
       " '가스불 켜놓고 나온거 같아',\n",
       " '가스비 너무 많이 나왔다 .',\n",
       " '가스비 비싼데 감기 걸리겠어',\n",
       " '가스비 장난 아님',\n",
       " '가장 확실한 건 뭘까 ?',\n",
       " '가족 여행 가기로 했어',\n",
       " '가족 여행 고고',\n",
       " '가족 여행 어디로 가지 ?',\n",
       " '가족 있어 ?',\n",
       " '가족관계 알려 줘',\n",
       " '가족끼리 여행간다 .',\n",
       " '가족들 보고 싶어',\n",
       " '가족들이랑 서먹해',\n",
       " '가족들이랑 서먹해졌어',\n",
       " '가족들이랑 어디 가지 ?',\n",
       " '가족들이랑 여행 갈거야',\n",
       " '가족여행 가야지',\n",
       " '가족이 누구야 ?',\n",
       " '가족이랑 여행 가려고',\n",
       " '가족한테 스트레스 풀었어',\n",
       " '가출할까 ?',\n",
       " '가출해도 갈 데가 없어',\n",
       " '간만에 떨리니까 좋더라',\n",
       " '간만에 쇼핑 중',\n",
       " '간만에 휴식 중',\n",
       " '간식 뭐 먹을까',\n",
       " '간식 추천',\n",
       " '간장치킨 시켜야지',\n",
       " '간접흡연 싫어',\n",
       " '갈까 말까 고민 돼',\n",
       " '갈까 말까 ?',\n",
       " '감 말랭이 먹고 싶다 .',\n",
       " '감 말랭이 먹어야지',\n",
       " '감기 같애',\n",
       " '감기 걸린 것 같아',\n",
       " '감기 기운이 있어',\n",
       " '감기 들 거 같애',\n",
       " '감기가 오려나',\n",
       " '감기약이 없어',\n",
       " '감기인거 같애',\n",
       " '감미로운 목소리 좋아',\n",
       " '감정이 쓰레기통처럼 엉망진창이야',\n",
       " '감정컨트롤을 못하겠어',\n",
       " '감정컨트롤이 안돼',\n",
       " '감히 나를 무시하는 애가 있어',\n",
       " '갑자기 나쁜 생각이 막 들더라',\n",
       " '갑자기 눈물 나',\n",
       " '갑자기 물어봐서 당황했어',\n",
       " '갑자기 불편한 사이가 된 거 같아',\n",
       " '강렬한 첫인상 남겨야 하는데',\n",
       " '강아지 키우고 싶어',\n",
       " '강아지 키우고 싶은데 역시 안돼겠지',\n",
       " '강아지 키울 수 있을까',\n",
       " '강아지 키울까',\n",
       " '강원도 가서 살까 ?',\n",
       " '같이 게임하자고 해도 되나 ?',\n",
       " '같이 놀러갈 친구가 없어',\n",
       " '같이 먹었는데 나만 살찐 거 같아',\n",
       " '같이 수영장 가기로 했어',\n",
       " '같이 있으면 힘든데 붙잡고 싶어',\n",
       " '같이 피씨방 가자고 해볼까 ?',\n",
       " '같이 할 수 있는 취미 생활 뭐 있을까',\n",
       " '개강룩 입어볼까',\n",
       " '개강옷 예쁘게 입어 볼까',\n",
       " '개강이다',\n",
       " '개강이라니',\n",
       " '개같은 상황',\n",
       " '개같이 되버렸어 .',\n",
       " '개기름 꼈어',\n",
       " '개념도 놓고 옴',\n",
       " '개념이 없어',\n",
       " '개당황',\n",
       " '개당황했잖아 갑자기 물어 봐서',\n",
       " '개인적인 업무까지 다 시켜',\n",
       " '개인적인 일도 다 시켜',\n",
       " '개졸려',\n",
       " '개좋아',\n",
       " '개학하니까 좋다',\n",
       " '걔 너무 싫다',\n",
       " '걔는 누굴 닮아서 그런거니 ?',\n",
       " '걔랑 같은 반 됐으면 좋겠다',\n",
       " '거지 같이 일해 놓고 갔어',\n",
       " '거지됐어',\n",
       " '거짓말 했어',\n",
       " '거짓말을 나도 모르게 자꾸 해',\n",
       " '거짓말을 하게 돼',\n",
       " '거짓말이 거짓말을 낳아',\n",
       " '걱정 없이 살고파',\n",
       " '걱정 좀 없이 살고 싶다 .',\n",
       " '건강 관리',\n",
       " '건강 빨리 회복해야지',\n",
       " '건강검진 왔어',\n",
       " '건강검진하러 옴',\n",
       " '건강이 최고',\n",
       " '건강이 최고인 것 같아',\n",
       " '건강하게 다이어트 하는 방법',\n",
       " '건강한 다이어트법',\n",
       " '건너건너 아는 사람인데 연락해도 될까 ?',\n",
       " '건물주 되고싶어',\n",
       " '건물주가 짱인데',\n",
       " '건방져',\n",
       " '건조기 살까봐',\n",
       " '건조하네',\n",
       " '걸레질도 해야 돼',\n",
       " '걸어 가고 있는데 깜깜해서 무서워',\n",
       " '겁난다',\n",
       " '게으른 동료가 있어',\n",
       " '게임 같이 하자고 할까 ?',\n",
       " '게임 때문에 시간 다갔어',\n",
       " '게임 때문에 폰이 점점 느려지는듯',\n",
       " '게임 재미있어 .',\n",
       " '게임 지겨워',\n",
       " '게임도 이제 재미없어',\n",
       " '게임하고 싶어',\n",
       " '게임하다 시간 다갔어',\n",
       " '겨울 지나 봄이야',\n",
       " '겨울에는 온천이지 !',\n",
       " '겨울이 가고 봄이 올거야',\n",
       " '격려 좀 해줘',\n",
       " '격려가 필요해 .',\n",
       " '견과류 챙겨 먹어야지 .',\n",
       " '결국 이런 운명이라니 슬프다',\n",
       " '결정 못하겠어 .',\n",
       " '결정은 빠르면 빠를 수록 좋겠지 ?',\n",
       " '결정은 빠를수록 좋겠지 ?',\n",
       " '결정을 못 내리겠어 . 어떻해',\n",
       " '결정적인 물증이 없어',\n",
       " '결혼 했는데 .',\n",
       " '결혼 했어',\n",
       " '결혼도 다 돈이다 .',\n",
       " '결혼식 가기 귀찮아',\n",
       " '결혼식 또 가야돼',\n",
       " '결혼식때 하객이 없을 까봐 걱정돼',\n",
       " '결혼식이 너무 많아',\n",
       " '결혼이나 하지 왜 자꾸 나한테 화 내냐구 !',\n",
       " '결혼준비 돈 많이 들겠지',\n",
       " '결혼준비하는데 돈 얼마나 드나',\n",
       " '결혼하는데 돈 많이 드네',\n",
       " '결혼하는데 돈 얼마나 들까',\n",
       " '결혼하면 좋아 ?',\n",
       " '결혼하면 좋을까',\n",
       " '결혼하면 행복할까 ?',\n",
       " '결혼하면 행복해 ?',\n",
       " '결혼하면 행복해질까 ?',\n",
       " '결혼할까',\n",
       " '결혼해도 되나',\n",
       " '결혼해도 될까',\n",
       " '결혼해야 하나',\n",
       " '경쟁이 너무 치열해',\n",
       " '계속 공부해도 될까',\n",
       " '계속 도전하는 거 귀찮아',\n",
       " '계속 방학이면 좋을텐데',\n",
       " '계속 보고 싶어',\n",
       " '계속 보고 싶으면 어떡해 ?',\n",
       " '계속 속이 진짜 안 좋아',\n",
       " '계속 엇갈리는 느낌',\n",
       " '계속 학생하고 싶어',\n",
       " '계속 한숨만 나와',\n",
       " '고3은 공부만 해야겠지 .',\n",
       " '고3이니까 공부해야겠지',\n",
       " '고구마 다이어트 해야지',\n",
       " '고구마만 먹고 다이어트 해야지',\n",
       " '고기 구워 먹고 싶다 .',\n",
       " '고기 먹고 싶어',\n",
       " '고데기 망했어',\n",
       " '고데기 했는데 망했어',\n",
       " '고독한 밤',\n",
       " '고마운 사람들이 많아',\n",
       " '고무신 거꾸로 신으면 어쩌지',\n",
       " '고민 있어',\n",
       " '고민 좀 들어줄래',\n",
       " '고백하고 후회하면 어떡하지',\n",
       " '고시원 너무 답답해',\n",
       " '고시원 답답해',\n",
       " '고시원에서 나가고 싶어',\n",
       " '고시원에서 탈출하고 싶어',\n",
       " '고양이 동영상 보는 중',\n",
       " '고양이 키우고 싶어',\n",
       " '고양이 키우고 싶어',\n",
       " '고의는 아닌데 실수를 한 거 같아',\n",
       " '고집 센 사람',\n",
       " '고집하고는',\n",
       " '골프 못 치는데',\n",
       " '골프 배워야 돼',\n",
       " '골프 어려워',\n",
       " '골프치러 가야돼',\n",
       " '곱창 먹고 싶어 .',\n",
       " '곱창 생각나',\n",
       " '공무원 괜찮겠지',\n",
       " '공무원 되고 싶다',\n",
       " '공무원 되면 좋겠다',\n",
       " '공무원 시험 공부 힘들다',\n",
       " '공무원 시험 죽을 거 같아',\n",
       " '공무원 시험 힘들어',\n",
       " '공무원 준비할까',\n",
       " '공무원이 좋지 ?',\n",
       " '공복이라 신경이 예민해져',\n",
       " '공복이라 예민해',\n",
       " '공복이면 예민함 ?',\n",
       " '공부 계속해도 될까',\n",
       " '공부 꼭 해야 할까',\n",
       " '공부 때려치워야 하나',\n",
       " '공부 시작해도 될까',\n",
       " '공부 왜 해야 돼 ?',\n",
       " '공부 잘 안돼',\n",
       " '공부 잘하고 싶어',\n",
       " '공부 좀 더 할 걸',\n",
       " '공부 하기 싫다',\n",
       " '공부는 내 체질이 아닌 것 같아',\n",
       " '공부로 먹고 살 수 있을까',\n",
       " '공부방법이 잘못된걸까 ?',\n",
       " '공부하기 싫어',\n",
       " '공부하기 싫은 날',\n",
       " '공부하는 낙이 없어',\n",
       " '공부하는 이유 ?',\n",
       " '공부하는 이유가 없어',\n",
       " '공시 준비 힘들어',\n",
       " '공시 준비 힘들어',\n",
       " '공시 준비중',\n",
       " '공시 준비하는데 힘들다',\n",
       " '공시생이야',\n",
       " '공연 보고 싶어',\n",
       " '공연 보러 가고 싶어',\n",
       " '공책 필기 나만 힘들어 ?',\n",
       " '공황장애 생겼어 .',\n",
       " '공황장애 있어',\n",
       " '공휴일에는 집이 최고',\n",
       " '공휴일에는 집콕',\n",
       " '과거는 잊고 앞으로 나아 가야지',\n",
       " '과거는 중요하지 않아',\n",
       " '과식해서 소화가 안돼',\n",
       " '과식했나 봐',\n",
       " '과식했다',\n",
       " '과외비 부담되겠지 ?',\n",
       " '과외비 비싸 ?',\n",
       " '과일 먹고 자야지',\n",
       " '과일 먹어야지 .',\n",
       " '과일 안 먹게 돼',\n",
       " '과일 잘 안 먹게 돼',\n",
       " '과일 챙겨 먹어야지',\n",
       " '관계가 계속 애매하다 .',\n",
       " '관심 끄라고 하고 싶다 .',\n",
       " '관심 좀 안 가졌으면',\n",
       " '관절염 같애',\n",
       " '관절염인가',\n",
       " '광고가 안 끝나',\n",
       " '괜찮아지고 있어',\n",
       " '괜찮은 사람인데 사귀긴 싫어',\n",
       " '괜히 건들지 말라고',\n",
       " '괜히 기다렸어',\n",
       " '괜히 농담해서 망했다',\n",
       " '괜히 아까운 시간 버렸다',\n",
       " '괜히 창피해',\n",
       " '괴물이 되어 가는 느낌이 들어',\n",
       " '교보문고 왔어',\n",
       " '교양 수업 재밌어',\n",
       " '교양수업 시간에 마음에 드는 애 있어',\n",
       " '교양수업 은근 재미져',\n",
       " '교양수업에서 마음에 드는 애 있어',\n",
       " '교양수업이 재미있어',\n",
       " '교양이 전공보다 재미있어',\n",
       " '교직이수 가능할까',\n",
       " '교통사고 났었어 .',\n",
       " '교통사고 당했어',\n",
       " '교회 가기 싫어',\n",
       " '교회 갔다 만났어',\n",
       " '교회에서 만났어',\n",
       " '구박하면서 엄청 일 시켜',\n",
       " '군대 갔다 올 때까지 기다릴 수 있을까',\n",
       " '군대 기다려 주려고',\n",
       " '군대 기다려도 될까',\n",
       " '군대 기다리면 부담스러워할까',\n",
       " '군대 기다릴 수 있을까',\n",
       " '군대 언제 끝나나',\n",
       " '군대 전역 기다려',\n",
       " '굿모닝',\n",
       " '궁금하면 오백원',\n",
       " '궁금하지 ?',\n",
       " '궁금해',\n",
       " '궁금해 알려줘',\n",
       " '귀 아파',\n",
       " '귀가 가려워',\n",
       " '귀가 간지러',\n",
       " '귀가 윙윙거려',\n",
       " '귀농 어때 ?',\n",
       " '그 사람이 나 안 좋아하는 거 같아',\n",
       " '그 사람이 나 좋아해줬으면 좋겠다',\n",
       " '그 사람이 행복했으면 좋겠다',\n",
       " '그 시절엔 다 그랬지',\n",
       " '그냥 고백할걸',\n",
       " '그냥 공무원이 좋을 듯',\n",
       " '그냥 내버려 둬 주었으면',\n",
       " '그냥 선 볼까 ?',\n",
       " '그냥 쉬고 싶다',\n",
       " '그냥 씹어야겠다 .',\n",
       " '그냥 이렇게 살고 싶어',\n",
       " '그냥 자는 거 아니지 ?',\n",
       " '그냥 잘못했다고 하면 될거 같은데 자꾸 변명해',\n",
       " '그냥 택시 타야지 .',\n",
       " '그냥 할까 ?',\n",
       " '그냥 혼자 밥이나 먹어야지',\n",
       " '그냥 혼자 있는게 좋아',\n",
       " '그동안 잘 지냈나요 ?',\n",
       " '그땐 그랬지',\n",
       " '그래 그러자',\n",
       " '그래 이제 결정했어',\n",
       " '그래도 좀 기대했는데',\n",
       " '그런 말을 왜 하지',\n",
       " '그런 사람인가보다 해야하나봐',\n",
       " '그런 사람인갑다 해야지',\n",
       " '그런 친구 아니었는데 너무 귀찮게 하네',\n",
       " '그렇게 갈 거면서',\n",
       " '그렇게 오래 살았는데도 이해를 못하겠어',\n",
       " '그렇게 할래',\n",
       " '그림 잘 그리고 싶다',\n",
       " '그림 좀 잘 그렸으면 좋겠다',\n",
       " '그만 두고 나오고 싶어',\n",
       " '그만 먹어야 하는데',\n",
       " '그만 살고싶어',\n",
       " '그저 그런 하루',\n",
       " '근사한 곳 알아 냈어',\n",
       " '근육 있으면 멋있을텐데',\n",
       " '금값 알아 ?',\n",
       " '금값 어때',\n",
       " '금사빠인가',\n",
       " '금수저 물고 태어나면 좋겠지 ?',\n",
       " '금수저로 태어났으면',\n",
       " '금수저로 태어났으면 좋았을텐데',\n",
       " '금연이 쉽지 않아',\n",
       " '기 빨렸어',\n",
       " '기념일 다 챙기는거 귀찮아',\n",
       " '기념일 또 까먹었어',\n",
       " '기념일 못챙겼어',\n",
       " '기념일 챙기기 귀찮아',\n",
       " '기능 좀 알려줘봐봐',\n",
       " '기다리는 것도 지쳐',\n",
       " '기다리라고 말 못하겠어',\n",
       " '기다림이 습관이 됐나봐',\n",
       " '기대가 무너졌어',\n",
       " '기대가 부담스러운데 떨쳐낼 수 있는 방법 있을까 ?',\n",
       " '기대하고 있었는데',\n",
       " '기대하지 말걸',\n",
       " '기대했는데',\n",
       " '기댈 수 있는 사람',\n",
       " '기력이 없어',\n",
       " '기름값 올랐어 .',\n",
       " '기본이 뭔지도 모르는 것 같아 .',\n",
       " '기본이 안 되어 있어',\n",
       " '기부 좀 했어요',\n",
       " '기부했어',\n",
       " '기분 꿀꿀해',\n",
       " '기분 나쁜 농담을 계속하고 있어',\n",
       " '기분 울적해서 좀 걷고 있어',\n",
       " '기분 전환 하고 싶어',\n",
       " '기분 전환이 필요해',\n",
       " '기분이 그지 같아',\n",
       " '기분이 더러워',\n",
       " '기분이 묘해',\n",
       " '기분이 이상해',\n",
       " '기숙사 괜찮을까',\n",
       " '기숙사 떨어졌어',\n",
       " '기숙사 사는거 어떨까 ?',\n",
       " '기숙사 살면 불편해 ?',\n",
       " '기숙사 안됐어',\n",
       " '기술 배울까',\n",
       " '기차 타고 여행 가고 싶어',\n",
       " '기차여행 가고 싶어',\n",
       " '기침도 못하겠어',\n",
       " '기침도 편하게 못해',\n",
       " '기프트콘 받았어 !',\n",
       " '기프트콘 선물 괜찮을까 ?',\n",
       " '기프트콘 선물해볼까 ?',\n",
       " '기프트콘 주면 좋아할까 ?',\n",
       " '기프트콘으로 선물 받았어',\n",
       " '기프트콘으로 선물 해야겠다',\n",
       " '기회를 놓쳤어',\n",
       " '기회를 못 잡았어',\n",
       " '기획사니까 당연히 예쁜 애들 많겠지',\n",
       " '기획사에 예쁜 애들 많겠지',\n",
       " '긴 머리 관리 어렵다 .',\n",
       " '긴 머리 관리하는 거 힘들다',\n",
       " '긴 시간이 걸렸지만 괜찮아 .',\n",
       " '긴장 푸는 법 알려줘',\n",
       " '긴장돼',\n",
       " '긴장돼서 땀나네',\n",
       " '길거리에서 연락처 물어보면 줘도 되나',\n",
       " '길에서 담배 피우는 사람 싫어',\n",
       " '길에서 번호 따였어',\n",
       " '길에서 전번 물어보면 줘도 되나',\n",
       " '길에서 헌팅 당했어',\n",
       " '길은 멀고 해는 진다',\n",
       " '길이 미끄러워서 미끄러질뻔했어',\n",
       " '길이 안보여',\n",
       " '길이 얼어서 미끄러질뻔했어',\n",
       " '길이 얼었어',\n",
       " '김떡순 먹고 싶어 .',\n",
       " '김치도 없네',\n",
       " '김치볶음밥 먹어야지',\n",
       " '김치볶음밥이나 만들어 먹어야지',\n",
       " '김치찌개 먹고 싶어',\n",
       " '까아 오빠들 컴백한다',\n",
       " '깜깜한데 전기 안들어오네',\n",
       " '깡 마른 거 같아',\n",
       " '꼴 사나워질 것 같은데',\n",
       " '꽃 받고 싶다',\n",
       " '꽃 사고 싶어',\n",
       " '꽃 살까 ?',\n",
       " '꽃 선물 좋아할까',\n",
       " '꽃 선물해 볼까',\n",
       " '꽃 예쁘게 말렸어',\n",
       " '꽃게탕 맛있다 .',\n",
       " '꽃게탕 진짜 밥도둑',\n",
       " '꽃꽂이 배우는 중',\n",
       " '꽃꽂이 배우니까 좋다',\n",
       " '꽃놀이 가고 싶어',\n",
       " '꽃다발 말려봐야지',\n",
       " '꽃다발 말리면 에쁘겠지 .',\n",
       " '꽃다발 받았어',\n",
       " '꽃다발 샀어',\n",
       " '꽃다발 선물 괜찮지 ?',\n",
       " '꽃다발 선물 받았어',\n",
       " '꽃다발 선물 어때 ?',\n",
       " '꽃다발 준비했어',\n",
       " '꽃바구니 선물이랑 과일 바구니 선물 뭐가 좋아 ?',\n",
       " '꽃바구니가 좋을까 과일바구니까 좋을까',\n",
       " '꽃선물 받고 어',\n",
       " '꿀잼',\n",
       " '꿈은 많은데',\n",
       " '꿈이 너무 많아',\n",
       " '꿈이 너무 무서웠어',\n",
       " '꿈이 다양해',\n",
       " '꿈이 두 개야',\n",
       " '꿈이 없어',\n",
       " '꿈이 이루어질까 ?',\n",
       " '꿈이 자꾸 바뀌어',\n",
       " '꿈이 현실이었으면',\n",
       " '끝나니까 허무하다',\n",
       " '끝나면 좋을 줄 알았는데 .',\n",
       " '낌새가 이상하더니 딱 걸렸어',\n",
       " '낌새가 있더니 딱 걸렸어',\n",
       " '나 감정쓰레기통이었나봐',\n",
       " '나 갖고 장난친건가',\n",
       " '나 같은 사람은 동물 키우면 안되겠지',\n",
       " '나 같이 예쁜 애를 왜 갈구지',\n",
       " '나 거짓말 못하겠어',\n",
       " '나 결정 잘 한거지 ?',\n",
       " '나 결정했어',\n",
       " '나 괜찮지 않니',\n",
       " '나 교직이수할 수 있을까 ?',\n",
       " '나 그동안 뭐한거니',\n",
       " '나 그지임',\n",
       " '나 내일 기숙사 가야돼',\n",
       " '나 내장비만이래',\n",
       " '나 너무 못 생겼어',\n",
       " '나 너무 소심해',\n",
       " '나 노트북 사줘',\n",
       " '나 놀려먹기 쉬운가 ?',\n",
       " '나 누구게 ?',\n",
       " '나 누락됐나봐',\n",
       " '나 다른 거 할까',\n",
       " '나 대충한 거 아닌데',\n",
       " '나 뒷담화하는 애 어떻게 할까 ?',\n",
       " '나 뒷담화하는 애 있다는데 어떻게 하지 ?',\n",
       " '나 많이 기대했는데',\n",
       " '나 말 실수한 거 같아 .',\n",
       " '나 맨날 속는 거 같아',\n",
       " '나 머리 나쁜 듯',\n",
       " '나 머리가 나뿐 것 같아',\n",
       " '나 먼저 잘게',\n",
       " '나 모르는게 왜 이렇게 많지',\n",
       " '나 몰래 사귀는 거 같애',\n",
       " '나 무시 당한 거 같아',\n",
       " '나 무시하는 거 같아',\n",
       " '나 무시하는 사람 어떻게 해 ?',\n",
       " '나 무시하는 사람 짜증나',\n",
       " '나 문제가 많은거 같아',\n",
       " '나 뭐하는 거지',\n",
       " '나 미팅한다 !',\n",
       " '나 바뀌고 싶어',\n",
       " '나 바본인가 봄',\n",
       " '나 백수야',\n",
       " '나 버림 받은 거 같아',\n",
       " '나 보이스피싱 당한 거 같은데 어떡해 ?',\n",
       " '나 비만이야',\n",
       " '나 사랑하니 ?',\n",
       " '나 상 받는대 !',\n",
       " '나 새 옷 샀다',\n",
       " '나 서류에서 광탈했어',\n",
       " '나 소개팅한다 !',\n",
       " '나 속은 거 같아',\n",
       " '나 속은듯',\n",
       " '나 수학여행 간다',\n",
       " '나 스마트폰 중독인가봐',\n",
       " '나 승진했어',\n",
       " '나 실수한건가',\n",
       " '나 실수했나',\n",
       " '나 아재인가',\n",
       " '나 아직 어른 아닌 거 같아',\n",
       " '나 아직도 애 같아 .',\n",
       " '나 어때 ?',\n",
       " '나 여기서 뭐하는 거지',\n",
       " '나 연기 너무 못해 거짓말 못하겠어',\n",
       " '나 열심히 할거야',\n",
       " '나 오늘 개불쌍',\n",
       " '나 오늘 따라 잘생겨 보이네',\n",
       " '나 오늘 상 받았지롱',\n",
       " '나 완전 계탔어 !',\n",
       " '나 왕따야',\n",
       " '나 왕따인거 같아',\n",
       " '나 왜 멍청해',\n",
       " '나 왜 이러지 ?',\n",
       " '나 왜케 못 생겼지',\n",
       " '나 요즘 정신 놓고 살고 있는 거 같아',\n",
       " '나 욕 먹는 거 같아',\n",
       " '나 웃겨 봐',\n",
       " '나 은근 무시하는 애 있어',\n",
       " '나 이상한가',\n",
       " '나 이상해 ?',\n",
       " '나 이제 졸업해',\n",
       " '나 인정받고 싶어',\n",
       " '나 잘 살 수 있겠지',\n",
       " '나 잘생겼지 ?',\n",
       " '나 잘하고 있는 건지 모르겠어',\n",
       " '나 잘하고 있는 걸까 ?',\n",
       " '나 잘하는 게 없어',\n",
       " '나 잘하는게 없는거같아',\n",
       " '나 잘할 수 있을까',\n",
       " '나 점점 괴물이 되고 있어',\n",
       " '나 정신차리게 말해줘',\n",
       " '나 좀 건들지 마',\n",
       " '나 좀 건들지 말라고 해',\n",
       " '나 좀 내버려 두면 좋겠어',\n",
       " '나 좀 내버려 뒀으면',\n",
       " '나 좀 안 건들였으면 좋겠어',\n",
       " '나 좀 좋아해줬으면',\n",
       " '나 좀 쩌는 듯',\n",
       " '나 좀 칭찬해줘',\n",
       " '나 좋아하게 만들고 싶다',\n",
       " '나 좋아하는 것 같아',\n",
       " '나 좋아해주는 사람 있겠지 ?',\n",
       " '나 주름살 있나 ?',\n",
       " '나 죽을 뻔함',\n",
       " '나 짤릴 거 같아',\n",
       " '나 쫌 불쌍한 거 같아',\n",
       " '나 챙겨줄 사람이 필요해',\n",
       " '나 천재 같아',\n",
       " '나 천재임',\n",
       " '나 축구는 진짜 잘해',\n",
       " '나 친구들한테 인정받고 싶어',\n",
       " '나 폭식증인듯',\n",
       " '나 폰 중독인 거 같애',\n",
       " '나 폰겜 너무 많이해',\n",
       " '나 폰겜했더니 몇 시간 갔어',\n",
       " '나 할 수 있어',\n",
       " '나 함부로 말하는 거 고치고 싶어',\n",
       " '나 혼자 야근해',\n",
       " '나 혼자 여행 왔는데 괜찮네',\n",
       " '나 혼자서 축구 본다',\n",
       " '나 화장을 너무 못해',\n",
       " '나 화장이 잘 안돼',\n",
       " '나 회사에서 인정받고 싶어',\n",
       " '나가기도 귀찮아',\n",
       " '나는 그냥저냥 사는 거 같아',\n",
       " '나는 기분 나쁜데 농담이라고 계속해',\n",
       " '나는 나약한 존재',\n",
       " '나는 누구인가',\n",
       " '나는 모자란 사람인 거 같아',\n",
       " '나는 뭐든 할 수 있다 .',\n",
       " '나는 뭘 잘할까',\n",
       " '나는 왜 이 모양일까',\n",
       " '나는 왜 이렇게 태어났을까 ?',\n",
       " '나는 왜 태어났을까',\n",
       " '나는 잘 할줄 아는 게 없는 것 같아',\n",
       " '나는 좋아하는 게 뭘까',\n",
       " '나는 좋은데   .',\n",
       " '나는 친구가 없어',\n",
       " '나는 친구라고 믿었는데',\n",
       " '나도 괜찮은 사람인데',\n",
       " '나도 대우 받고 싶다고',\n",
       " '나도 비키니 입고 싶다',\n",
       " '나도 상 받고 싶다',\n",
       " '나도 약초 캐볼까 ?',\n",
       " '나도 월급 필요해',\n",
       " '나도 위로 받고 싶다',\n",
       " '나도 이벤트가 되다니 !',\n",
       " '나도 이제 아재인가',\n",
       " '나도 중국 진출해볼까 ?',\n",
       " '나도 집 사고 싶어',\n",
       " '나도 커플룩 입고 싶다',\n",
       " '나두 잘할거야',\n",
       " '나들이를 가볼까',\n",
       " '나란 놈',\n",
       " '나랑 놀아줘',\n",
       " '나랑 놀자',\n",
       " '나랑 상관 없는 이야기들',\n",
       " '나랑 있는게 힘들었나봐',\n",
       " '나른하다',\n",
       " '나를 기다려줬으면 좋겠다',\n",
       " '나를 너무 오래 기다리게했어',\n",
       " '나를 너무 함부로 대해',\n",
       " '나를 미소짓게 만든 너',\n",
       " '나를 바꿀 수 있는 건 뭐가 있을까',\n",
       " '나를 친구로 생각 안했나봐',\n",
       " '나를 호구로 아는 사람 어떡해 ?',\n",
       " '나를 힘들게 하는 사람인데 붙잡고 싶어',\n",
       " '나만 갈궈',\n",
       " '나만 기다렸나봐',\n",
       " '나만 꿈 없이 사는 거 같아',\n",
       " '나만 남친 없어',\n",
       " '나만 뒤처지는 느낌이야',\n",
       " '나만 반친구 없어',\n",
       " '나만 빼고 행복해보여',\n",
       " '나만 설레나',\n",
       " '나만 설레는 거야',\n",
       " '나만 솔로야',\n",
       " '나만 애기봐',\n",
       " '나만 야근해',\n",
       " '나만 우스워질거 같아',\n",
       " '나만 이상한 사람이래',\n",
       " '나만 이상해졌어',\n",
       " '나만 일시켜서 짜증폭발',\n",
       " '나만 제자리걸음이야',\n",
       " '나만 제자리인듯',\n",
       " '나만 진급 못했어',\n",
       " '나만 친구라고 생각한건가',\n",
       " '나만 친구로 생각했나봐',\n",
       " '나만 힘든 거 아니지 ?',\n",
       " '나만의 시간이 필요한 것 같아',\n",
       " '나만의 시간이 필요해',\n",
       " '나빼고 다 행복한 거 같아',\n",
       " '나쁜 꿈 꿨어',\n",
       " '나이 때문에 무시 받았어',\n",
       " '나이 어리다고 무시해',\n",
       " '나이가 많은데 취직이 될까',\n",
       " '나이도 있으니 영양제 좀 챙겨볼까',\n",
       " '나이들면서 눈물이 많아졌어',\n",
       " '나이먹으니까 주름살 생겨',\n",
       " '나중에 뭐하고 먹고 사냐',\n",
       " '나중에 뭐할까 고민이야',\n",
       " '나중에 창업해야 겠지',\n",
       " '나한테 감추는 게 하나도 없었으면',\n",
       " '나한테 거짓말 좀 안 했으면',\n",
       " '나한테 냄새 나면 어쩌지 ?',\n",
       " '나한테 냄새 날까 ?',\n",
       " '나한테 너무 많은 걸 바라는 듯',\n",
       " '나한테 문제가 많아',\n",
       " '나한테 상의 좀 하지',\n",
       " '나한테 상의하면 좋을텐데',\n",
       " '나한테 이상한 냄새 나나 ?',\n",
       " '나한테 할 말 있대 뭘까 ?',\n",
       " '나한테 행운 좀 왔으면 좋겠어',\n",
       " '나한테만 예의 차리래',\n",
       " '나한테만 왜 이런 일이 일어날까',\n",
       " '나한테만은 완전 솔직했으면',\n",
       " '낙엽 밟는 소리 좋다',\n",
       " '낙엽밟는 소리',\n",
       " '낚시 안 해봤는데',\n",
       " '낚시 안 해봤는데 재미있어 보인다',\n",
       " '낚시 재밌을까',\n",
       " '낚시 좋아하는 남자 어때 ?',\n",
       " '낚시는 무슨 재미 ?',\n",
       " '난 동물 못키울거 같아',\n",
       " '난 많이 노력한 거 같은데',\n",
       " '난 쓰레기야',\n",
       " '난 왜 예쁘게 말을 못할까',\n",
       " '난 왜 이모양일까',\n",
       " '난 정말 안되겠다',\n",
       " '난 진짜 쓰레기야',\n",
       " '난 천재다',\n",
       " '난방비 비싼데 추워',\n",
       " '난방이 안돼',\n",
       " '난방이 안돼나 추워',\n",
       " '날 몇시간동안이나 기다리게했어',\n",
       " '날씨 건조한 거 같애',\n",
       " '날씨 왜 이렇게 춥냐',\n",
       " '날씨 좀 풀린거 같아',\n",
       " '날씨 좋은데',\n",
       " '날씨 죽인다',\n",
       " '날씨 짱 좋아',\n",
       " '날씨 풀렸다',\n",
       " '날씨가 너무 눅눅해',\n",
       " '날씨가 너무 추워',\n",
       " '날씨가 북극같아',\n",
       " '날씨가 진짜 덥다',\n",
       " '날아 가고 싶어',\n",
       " '남동생한테 자꾸 화내게 되네',\n",
       " '남들에게 인정받으려면 어떻게 해야 돼 ?',\n",
       " '남들이 날 욕하는 거 같아',\n",
       " '남들이 다 손가락질 하는 거 같아',\n",
       " '남은 휴가가 없어',\n",
       " '남의 눈을 너무 신경써',\n",
       " '남의 일 도와줘야 할까',\n",
       " '남의 차 긁었어 내 돈',\n",
       " '남이 걷지 않는 길을 가려고 해',\n",
       " '남자 보통 어디서 만나',\n",
       " '남자 어디서 만나',\n",
       " '남자 친구가 바래다 줬어',\n",
       " '남자 화장하는 거 어때',\n",
       " '남자가 낚시를 너무 좋아해',\n",
       " '남자가 화장하는 거 어떻게 생각해',\n",
       " '남자면 편할 것 같아',\n",
       " '남자였으면 좋겠어',\n",
       " '남자인지 여자인지 알려줘',\n",
       " '남자친구 교회 데려가고 싶어',\n",
       " '남자친구 또 운동 갔어',\n",
       " '남자친구 생일인데 뭘 줄까',\n",
       " '남자친구 승진 선물로 뭐가 좋을까 ?',\n",
       " '남자친구 오늘 따라 훈훈해 보인다',\n",
       " '남자친구 오늘 좀 질린다 .',\n",
       " '남자친구가 나 안 믿어줘',\n",
       " '남자친구가 너무 바빠',\n",
       " '남자친구가 너무 운동만 해',\n",
       " '남자친구가 너무 잘생겼어',\n",
       " '남자친구가 데려다줬어',\n",
       " '남자친구가 맞춤법을 너무 많이 틀려',\n",
       " '남자친구가 사업 시작한대',\n",
       " '남자친구가 사업한대',\n",
       " '남자친구가 사진 실력 꽝',\n",
       " '남자친구가 사진을 너무 못 찍어',\n",
       " '남자친구가 안놀아 줘',\n",
       " '남자친구가 애교가 많아',\n",
       " '남자친구가 욕함',\n",
       " '남자친구가 의심해',\n",
       " '남자친구가 이벤트 해 주면 좋겠다 .',\n",
       " '남자친구가 이벤트를 잘 안해줘',\n",
       " '남자친구가 입이 험해',\n",
       " '남자친구가 자꾸 잔소리해',\n",
       " '남자친구가 잔소리가 심해',\n",
       " '남자친구가 전화를 잘 안해',\n",
       " '남자친구가 전화하는 걸 안 좋아해',\n",
       " '남자친구가 홧김에 욕함',\n",
       " '남자친구는 어디서 만나',\n",
       " '남자친구랑 봉사활동 해보려고',\n",
       " '남자친구랑 종교 문제로 다툼',\n",
       " '남자친구랑 종교가 달라',\n",
       " '남자친구한테 질린 거 같아',\n",
       " '남친 sns에 내 사진 없어',\n",
       " '남친 때문에 살찐 듯',\n",
       " '남친 보여줄까',\n",
       " '남친 생일선물 뭘 주면 좋을까',\n",
       " '남친 승진 선물 추천',\n",
       " '남친 어디서 만나',\n",
       " '남친 프로필에 내 사진 왜 안올릴까',\n",
       " '남친 프사에 내 사진 없어',\n",
       " '남친이 sns에 내 사진에 안 올려',\n",
       " '남친이 입이 험해',\n",
       " '남친한테 교회 가자고 하고 싶어',\n",
       " '남편이 나 안 도와줘',\n",
       " '남편이 나보다 집안일 더 잘해',\n",
       " '남편이 맨날 늦게 들어와',\n",
       " '남편이 미워',\n",
       " '남편이 아기를 안 돌봐줘 .',\n",
       " '남편이 왜 애키우는거 안 도와줄까',\n",
       " '남편이 육아를 안해',\n",
       " '남편이 육아에 무신경해',\n",
       " '남편이 집안일 안 도와줘 .',\n",
       " '남편이 집안일 안 해',\n",
       " '남편이 집안일을 너무 잘해',\n",
       " '남편이 짜증나게해',\n",
       " '남편이 하나도 안 도와줘',\n",
       " '남편이 회식이라고 안와',\n",
       " '남편이 회식하면 늦게 들어와',\n",
       " '낭만이 사라진 것 같아',\n",
       " '낭만이 없어',\n",
       " '낭만이라고는 없어가지구',\n",
       " '내 남자친구 보고 싶어 ?',\n",
       " '내 남자친구 아이돌이면 좋겠다 .',\n",
       " '내 능력이 너무 모자라',\n",
       " '내 마음을 알아줬으면',\n",
       " '내 마음을 좀 알아 달라고',\n",
       " '내 몸이 여러 개 였으면 좋겠다',\n",
       " '내 문제는 뭘까',\n",
       " '내 문제점이 뭘까',\n",
       " '내 배우자는 어디 있을까',\n",
       " '내 배우자도 어디 있을까 ?',\n",
       " '내 사수 너무 깐깐해',\n",
       " '내 생각대로 살거야',\n",
       " '내 생각이랑 다른 사람 생각이 진짜 다르다는 걸 느껴',\n",
       " '내 성격 너무 소심해',\n",
       " '내 스타일 아니던데',\n",
       " '내 스타일 아니야',\n",
       " '내 실력 좀 쩌는 듯',\n",
       " '내 얼굴이 읽히나',\n",
       " '내 여자친구 아이돌이야',\n",
       " '내 외모 맘에 안들어',\n",
       " '내 월급만 안 올라',\n",
       " '내 의견 좀 존중해 줬으면',\n",
       " '내 의견을 존중해줬으면',\n",
       " '내 의지는 상관없나봐',\n",
       " '내 의지로 안되는 일인가봐',\n",
       " '내 이름이 없어',\n",
       " '내 인생 답 없어',\n",
       " '내 인생은 가시밭길 같아',\n",
       " '내 인생의 주인공은 나야',\n",
       " '내 일 아닌데 해야 돼 ?',\n",
       " '내 자존감',\n",
       " '내 잘못이 뭔지 모르겠어',\n",
       " '내 잘못인 거 같은데 말을 못하겠어',\n",
       " '내 잘못인 거 같은데 어떻게 털어놓지',\n",
       " '내 주제를 모르고 덤빈건가',\n",
       " '내 지인한테 내 험담했대',\n",
       " '내 집이 생겼어',\n",
       " '내 짝은 어디있을까',\n",
       " '내 친구에게 내 험담을 하다니',\n",
       " '내 키 맞춰 봐',\n",
       " '내 키가 몇이게 ?',\n",
       " '내 편이 없는 거 같아',\n",
       " '내 편이라고는 하나도 없는 거 같아',\n",
       " '내가 그렇게 부족한가',\n",
       " '내가 그르친 거 같아',\n",
       " '내가 그사람이랑 진짜 결혼해도 될까',\n",
       " '내가 기대를 너무 많이 했나봐',\n",
       " '내가 나빴네',\n",
       " '내가 너무 방심했어',\n",
       " '내가 너무 생각없이 말했어',\n",
       " '내가 너무 쉽게 보였나 ?',\n",
       " '내가 너무 초라해',\n",
       " '내가 다른 무슨 말을 하겠어',\n",
       " '내가 만족을 못해',\n",
       " '내가 많이 부족한가',\n",
       " '내가 말하면 왜 비난만 할까',\n",
       " '내가 멍청한거지',\n",
       " '내가 무능력하게 느껴져',\n",
       " '내가 뭘 잘못했을까',\n",
       " '내가 뭘 좋아하는지 잘하는지 모르겠어',\n",
       " '내가 바보지',\n",
       " '내가 부족하니까 이렇게 밖에 안된거겠지 .',\n",
       " '내가 불효녀야',\n",
       " '내가 불효자야',\n",
       " '내가 사랑할 자격이 있나',\n",
       " '내가 쉬워보이나 ?',\n",
       " '내가 쓸모없는 인간 같아',\n",
       " '내가 아무것도 아닌 사람 같아',\n",
       " '내가 왜 해야하는지 모르겠어',\n",
       " '내가 원하는 사람이 되기 어려워',\n",
       " '내가 이래뵈도 괜찮은 사람인데',\n",
       " '내가 이렇게 또 불효를 한다 .',\n",
       " '내가 이상한 건가 ?',\n",
       " '내가 이상한 사람같아',\n",
       " '내가 이상한가 ?',\n",
       " '내가 잘못한 걸까',\n",
       " '내가 잘못했다는데 뭔지 안 알려줘',\n",
       " '내가 제일 문제인 듯',\n",
       " '내가 제정신이 아니다',\n",
       " '내가 좋아하는 가수 컴백한다',\n",
       " '내가 좋아하는 거 모르나',\n",
       " '내가 좋아하는 거 모르는 거 같애',\n",
       " '내가 좋아하는 사람과 나를 좋아해주는 사람',\n",
       " '내가 좋아하는 사람이 나 안 좋아하는 거 같아',\n",
       " '내가 좋아하는 사람이 나 좋아해줬으면 좋겠다',\n",
       " '내가 좋아하는 사람이 행복했으면 좋겠다',\n",
       " '내가 좋아할 자격이 있나',\n",
       " '내가 주제를 몰랐나봐',\n",
       " '내가 주제를 몰랐던 거지',\n",
       " '내가 죽어도 모를 거 같아',\n",
       " '내가 진짜 즐길 수 있을게 뭘까',\n",
       " '내가 질린대',\n",
       " '내가 참 못난거 같아',\n",
       " '내가 호구냐구',\n",
       " '내가 희생양이 됐어',\n",
       " '내가 힘든 게 많다',\n",
       " '내기해서 이겼는데 소원 뭐하지',\n",
       " '내년에는 더 행복해질려고 이렇게 힘든가봅니다',\n",
       " '내마음을 모르겠어 .',\n",
       " '내사랑은 어디 있나',\n",
       " '내아파트 갖고 싶어 .',\n",
       " '내일 기대하게 되네',\n",
       " '내일 기숙사 들어가',\n",
       " '내일 날씨 어때 ?',\n",
       " '내일 날씨 좋을까 ?',\n",
       " '내일 떨린다',\n",
       " '내일 만나자고 데쉬 ?',\n",
       " '내일 만나자고 해볼까 ?',\n",
       " '내일 모의고사 본다',\n",
       " '내일 모의평가다',\n",
       " '내일 발표 나는데 떨려',\n",
       " '내일 발표 준비 아자아자',\n",
       " '내일 발표 준비하고 있어',\n",
       " '내일 발표인데 떨려',\n",
       " '내일 비왔으면',\n",
       " '내일 소풍간다',\n",
       " '내일 수학여행가 !',\n",
       " '내일 시험이야',\n",
       " '내일 약속 있는데 날씨 좋았으면',\n",
       " '내일 일찍 일어나야 돼',\n",
       " '내일 친구랑 놀까 ?',\n",
       " '내일 클스마스 이브네 .',\n",
       " '내일 하루 종일 바빠',\n",
       " '내일은 기다리던 소풍 간다',\n",
       " '내일은 비왔으면 좋겠다 .',\n",
       " '내일은 친구들랑 놀까 ?',\n",
       " '내일이 기대돼',\n",
       " '내일이면 크리스마스 이브네 .',\n",
       " '내장 비만',\n",
       " '낼 데이트하기로했는데 날씨 좋았으면',\n",
       " '낼 바쁘넹',\n",
       " '냄새 나면 어쩌지 ?',\n",
       " '냄새나면 어쩌지',\n",
       " '냄새날 것 같아 걱정이야',\n",
       " '냉면 땡긴다',\n",
       " '냉방비 너무 많이 나와',\n",
       " '냉방비 장난 아님',\n",
       " '냉장고 털어도 먹을게 없네',\n",
       " '냉장고가 텅비었어',\n",
       " '냉장고에 김치도 없네',\n",
       " '냉장고에 먹을 게 없네',\n",
       " '냉장고에 먹을 게 하나도 없네',\n",
       " '너 누구 ?',\n",
       " '너 누구냐',\n",
       " '너 누구니 ?',\n",
       " '너 때문이야',\n",
       " '너 또 뭐할 줄 알아 ?',\n",
       " '너 만든 사람 최소 천재',\n",
       " '너 만든 사람은 누구야 ?',\n",
       " '너 말 잘하니',\n",
       " '너 말 제대로 못해 ?',\n",
       " '너 말이 좀 이상하다',\n",
       " '너 무서워',\n",
       " '너 뭐하는 애야',\n",
       " '너 미워',\n",
       " '너 이러면 미워한다',\n",
       " '너 진짜 쓰레기야',\n",
       " '너는 못가잖아',\n",
       " '너는 뭐 억었어 ?',\n",
       " '너는 아무일도 없었나봐 ?',\n",
       " '너는 안자 ?',\n",
       " '너덜너덜해진 느낌이야',\n",
       " '너도 고민 있니',\n",
       " '너도 고민 있어 ?',\n",
       " '너도 몰랐니',\n",
       " '너도 무슨 고민 있니',\n",
       " '너도 상사 있어',\n",
       " '너무 기빨려',\n",
       " '너무 기대했나봐',\n",
       " '너무 다른 문화인 듯',\n",
       " '너무 단순한 것만 하는거 아니니 .',\n",
       " '너무 더워',\n",
       " '너무 더워서 미치겠어',\n",
       " '너무 마른 거 같아',\n",
       " '너무 많은 걸 바래',\n",
       " '너무 많이 먹어서 소화시켜야 하는데 움직이기가 싫어',\n",
       " '너무 많이 먹었나봐',\n",
       " '너무 많이 먹었어',\n",
       " '너무 멋있다',\n",
       " '너무 바빠',\n",
       " '너무 배가 불러',\n",
       " '너무 불공평한거 같애',\n",
       " '너무 빨리 대답해',\n",
       " '너무 빨리 철 든 거 같아서 마음이 아파',\n",
       " '너무 빨리 철 들었어',\n",
       " '너무 뻔뻔하게 구는데',\n",
       " '너무 어려워',\n",
       " '너무 오래 기다리게 한다 .',\n",
       " '너무 외로워',\n",
       " '너무 잘하는 후배가 들어왔어',\n",
       " '너무 졸려',\n",
       " '너무 초라해지는 느낌이야',\n",
       " '너무 추워서 나가기 귀찮아',\n",
       " '너무 추워서 시베리아 같아',\n",
       " '너무 편해도 안 좋아',\n",
       " '너무 편해진 거 같아',\n",
       " '너무 허기지네',\n",
       " '너무 힘들다',\n",
       " '너무 힘들다 . 지쳤어 .',\n",
       " '너무하네 진짜',\n",
       " '넌 고민이 뭐야',\n",
       " '넌 누구냐 ?',\n",
       " '넘 많이 먹었다 .',\n",
       " '넘넘 외로워 죽겠어',\n",
       " '넘어져서 발목 삔 거 같애',\n",
       " '넘어질 뻔했어',\n",
       " '넘어질뻔했어',\n",
       " '네일 할까',\n",
       " '넥타이핀 선물 괜찮겠지 ?',\n",
       " '넥타이핀 정도는 선물로 줘도 괜찮겠지 ?',\n",
       " '노는게 제일 좋아',\n",
       " '노래 못해서 노래방 안 가',\n",
       " '노래 잘 부르는 사람 부러워',\n",
       " '노래 잘하고 싶어',\n",
       " '노래 잘하는 사람 부러워',\n",
       " '노래방 가고 싶어',\n",
       " '노래방 가면 어색할까',\n",
       " ...]"
      ]
     },
     "execution_count": 604,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "id": "8d1f4190",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 605,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(questions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "id": "5fcb5a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정수 인코딩, 최대 길이를 초과하는 샘플 제거, 패딩\n",
    "def tokenize_and_filter(inputs, outputs):\n",
    "  tokenized_inputs, tokenized_outputs = [], []\n",
    "  \n",
    "  for (sentence1, sentence2) in zip(inputs, outputs):\n",
    "    # 정수 인코딩 과정에서 시작 토큰과 종료 토큰을 추가\n",
    "    sentence1 = START_TOKEN + tokenizer.encode(sentence1) + END_TOKEN\n",
    "    sentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN\n",
    "\n",
    "    # 최대 길이 50 이하인 경우에만 데이터셋으로 허용\n",
    "    if len(sentence1) <= MAX_LENGTH and len(sentence2) <= MAX_LENGTH:\n",
    "      tokenized_inputs.append(sentence1)\n",
    "      tokenized_outputs.append(sentence2)\n",
    "  \n",
    "  # 최대 길이 50으로 모든 데이터셋을 패딩\n",
    "  tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "      tokenized_inputs, maxlen=MAX_LENGTH, padding='post')\n",
    "  tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "      tokenized_outputs, maxlen=MAX_LENGTH, padding='post')\n",
    "  \n",
    "  return tokenized_inputs, tokenized_outputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "id": "941a42cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions, answers = tokenize_and_filter(questions, answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "id": "2b91f553",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8164, 7902, 4198, ...,    0,    0,    0],\n",
       "       [8164, 7957,   47, ...,    0,    0,    0],\n",
       "       [8164, 7959, 1433, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [8164, 8145, 8065, ...,    0,    0,    0],\n",
       "       [8164,  134,  166, ...,    0,    0,    0],\n",
       "       [8164, 1951,  881, ...,    0,    0,    0]], dtype=int32)"
      ]
     },
     "execution_count": 608,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "id": "79ee8e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어장의 크기 : 8166\n",
      "필터링 후의 질문 샘플 개수: 11823\n",
      "필터링 후의 답변 샘플 개수: 11823\n"
     ]
    }
   ],
   "source": [
    "print('단어장의 크기 :',(VOCAB_SIZE))# 50개 이하로 잘라도 Vocab에 변화는 없음 (vocab다양성에 상관x)\n",
    "print('필터링 후의 질문 샘플 개수: {}'.format(len(questions)))\n",
    "print('필터링 후의 답변 샘플 개수: {}'.format(len(answers)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "id": "5d24facd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q. shuffle로 인해 cache의 locality가 깨지면"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "id": "2439e0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 20000\n",
    "\n",
    "# 디코더는 이전의 target을 다음의 input으로 사용합니다.\n",
    "# 이에 따라 outputs에서는 START_TOKEN을 제거하겠습니다.\n",
    "dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {\n",
    "        'inputs': questions,\n",
    "        'dec_inputs': answers[:, :-1]\n",
    "    },\n",
    "    {\n",
    "        'outputs': answers[:, 1:]\n",
    "    },\n",
    "))\n",
    "\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "id": "01ccd0a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: ({inputs: (None, 50), dec_inputs: (None, 49)}, {outputs: (None, 49)}), types: ({inputs: tf.int32, dec_inputs: tf.int32}, {outputs: tf.int32})>"
      ]
     },
     "execution_count": 618,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf7389d",
   "metadata": {},
   "source": [
    "## Model modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "id": "a751ee2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, position, d_model):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.pos_encoding = self.positional_encoding(position, d_model) #문장 길이(토큰 개수)임베딩 차원(단어 벡터 차원)\n",
    "\n",
    "    def get_angles(self, position, i, d_model):\n",
    "        angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
    "        return position * angles\n",
    "\n",
    "    def positional_encoding(self, position, d_model):\n",
    "        # 각도 배열 생성\n",
    "        angle_rads = self.get_angles(\n",
    "            position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
    "            i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
    "            d_model=d_model)\n",
    "\n",
    "        # 배열의 짝수 인덱스에는 sin 함수 적용\n",
    "        sines = tf.math.sin(angle_rads[:, 0::2]) # 0::2는 열 인덱스에서 0번부터 시작해서 2씩 건너뛰며 \n",
    "        # 배열의 홀수 인덱스에는 cosine 함수 적용\n",
    "        cosines = tf.math.cos(angle_rads[:, 1::2])\n",
    "\n",
    "        # sin과 cosine이 교차되도록 재배열\n",
    "        pos_encoding = tf.stack([sines, cosines], axis=0)\n",
    "        pos_encoding = tf.transpose(pos_encoding,[1, 2, 0]) \n",
    "        pos_encoding = tf.reshape(pos_encoding, [position, d_model])\n",
    "\n",
    "        pos_encoding = pos_encoding[tf.newaxis, ...] # tf.newaxis는 텐서의 첫 번째 차원에 새로운 축을 추가\n",
    "        return tf.cast(pos_encoding, tf.float32)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "id": "418da756",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(query, key, value, mask):\n",
    "    # 어텐션 가중치는 Q와 K의 닷 프로덕트\n",
    "    matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
    "\n",
    "    # 가중치를 정규화\n",
    "    depth = tf.cast(tf.shape(key)[-1], tf.float32) #tf.cast는 텐서의 데이터 타입을 변경하는 함수\n",
    "    logits = matmul_qk / tf.math.sqrt(depth)\n",
    "\n",
    "    # 패딩에 마스크 추가\n",
    "    if mask is not None:\n",
    "        logits += (mask * -1e9)\n",
    "\n",
    "    # softmax적용\n",
    "    attention_weights = tf.nn.softmax(logits, axis=-1)\n",
    "\n",
    "    # 최종 어텐션은 가중치와 V의 닷 프로덕트\n",
    "    output = tf.matmul(attention_weights, value)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "id": "29fc1741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (0)을 찾아서, 해당 위치에 마스크를 생성\n",
    "def create_padding_mask(x):\n",
    "    mask = tf.cast(tf.math.equal(x, 0), tf.float32)\n",
    "    # tf.newaxis를 사용하여 (batch_size, 1, 1, sequence length)형태로 변경합니다. (마스크 적용을 위함)\n",
    "    return mask[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "# 미래 토큰을 가리는 마스크\n",
    "def create_look_ahead_mask(x):\n",
    "    seq_len = tf.shape(x)[1] # x 텐서의 시퀀스 길이(두 번째 차원)를 seq_len 변수에 저장합니다.\n",
    "    look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "    padding_mask = create_padding_mask(x) # create_padding_mask 함수를 사용하여 padding mask를 생성합니다.\n",
    "    return tf.maximum(look_ahead_mask, padding_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "id": "154220a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n",
    "        super(MultiHeadAttention, self).__init__(name=name)\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.query_dense = tf.keras.layers.Dense(units=d_model)\n",
    "        self.key_dense = tf.keras.layers.Dense(units=d_model)\n",
    "        self.value_dense = tf.keras.layers.Dense(units=d_model)\n",
    "        self.dense = tf.keras.layers.Dense(units=d_model) # 단어 벡터는 attention 계산에 더 적합한 형태?\n",
    "\n",
    "    # 각 헤드는 서로 다른 의미나 관계를 학습\n",
    "    def split_heads(self, inputs, batch_size):\n",
    "        inputs = tf.reshape(inputs, shape=(batch_size, -1, self.num_heads, self.depth))  # (batch_size, seq_len, d_model) -> (batch_size, seq_len, num_heads, depth)\n",
    "        return tf.transpose(inputs, perm=[0, 2, 1, 3]) # (batch_size, num_heads, seq_len, depth)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        query, key, value, mask = inputs['query'], inputs['key'], inputs['value'], inputs['mask']\n",
    "        batch_size = tf.shape(query)[0]\n",
    "\n",
    "        # Q, K, V에 각각 Dense를 적용합니다\n",
    "        # normal embedding: (batch_size, seq_len, embedding_dim)\n",
    "        # attention required: d_model이 num_heads * depth\n",
    "        # Thus, Dense를 사용해 차원을 (batch_size, seq_len, d_model)로 맞춤\n",
    "        query = self.query_dense(query)\n",
    "        key = self.query_dense(key) \n",
    "        value = self.query_dense(value )\n",
    "        \n",
    "#         print(f'shape of query:{query.shape}, shape of key:{key.shape}, shape of value:{value.shape}')\n",
    "        #shape of query:(None, None, 128), shape of key:(None, None, 128), shape of value:(None, None, 128)\n",
    "        query = self.split_heads(query, batch_size)  # (batch_size, seq_len, d_model)\n",
    "        key = self.split_heads(key, batch_size)\n",
    "        value =self.split_heads(value, batch_size) \n",
    "        \n",
    "        #TODO: 찍어보기\n",
    "#         print(f'shape of query:{query.shape}, shape of key:{key.shape}, shape of value:{value.shape}')\n",
    "        #shape of query:(None, 8, None, 16), shape of key:(None, 8, None, 16), shape of value:(None, 8, None, 16) \n",
    "\n",
    "        scaled_attention = scaled_dot_product_attention(query, key, value, mask)\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "\n",
    "        # 어텐션 연산 후에 각 결과를 다시 연결(concatenate)합니다\n",
    "        concat_attention = tf.reshape(scaled_attention,(batch_size, -1, self.d_model))\n",
    "\n",
    "        # 최종 결과에도 Dense를 한 번 더 적용합니다\n",
    "        # 여러 Head의 결과를 이어붙이면 차원이 d_model보다 큼.\n",
    "        # Dense 레이어를 통해 \n",
    "        # (batch_size, seq_len, num_heads * depth) -> (batch_size, seq_len, d_model)로 차원을 복구\n",
    "        outputs = self.dense(concat_attention)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd45717",
   "metadata": {},
   "source": [
    "## Model Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "id": "ee7d9b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코더 하나의 레이어를 함수로 구현.\n",
    "# 이 하나의 레이어 안에는 두 개의 서브 레이어가 존재합니다.\n",
    "def encoder_layer(units, d_model, num_heads, dropout, name=\"encoder_layer\"):\n",
    "  inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\") # None: sequence_length는 모델 학습 시 유동적으로 변할 수 있음을 의미합니다.\n",
    "  \n",
    "  # 패딩 마스크 사용\n",
    "  padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\") # 1: 패딩 마스크는 각 문장에 대해 하나씩 생성되므로, 두 개의 차원은 1입니다.\n",
    "\n",
    "\n",
    "  # 첫 번째 서브 레이어 : 멀티 헤드 어텐션 수행 (셀프 어텐션)\n",
    "  attention = MultiHeadAttention(\n",
    "      d_model, num_heads, name=\"attention\")({\n",
    "          'query': inputs,\n",
    "          'key': inputs,\n",
    "          'value': inputs,\n",
    "          'mask': padding_mask\n",
    "      })\n",
    "\n",
    "  # 어텐션의 결과는 Dropout과 Layer Normalization이라는 훈련을 돕는 테크닉을 수행\n",
    "  attention = tf.keras.layers.Dropout(rate=dropout)(attention)\n",
    "  attention = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(inputs + attention)\n",
    "\n",
    "\n",
    "# '''\n",
    "# Dense는 각 헤드 정보 통합\n",
    "# '''\n",
    "  # 두 번째 서브 레이어 : 2개의 완전연결층\n",
    "  outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention)\n",
    "  outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "\n",
    "  # 완전연결층의 결과는 Dropout과 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n",
    "  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "  outputs = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(attention + outputs)\n",
    "\n",
    "  return tf.keras.Model(\n",
    "      inputs=[inputs, padding_mask], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "id": "4bc9a86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(vocab_size,\n",
    "            num_layers,\n",
    "            units,\n",
    "            d_model,\n",
    "            num_heads,\n",
    "            dropout,\n",
    "            name=\"encoder\"):\n",
    "    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "\n",
    "    # 패딩 마스크 사용\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "    # 임베딩 레이어\n",
    "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "\n",
    "    # 포지셔널 인코딩\n",
    "    embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "    # num_layers만큼 쌓아올린 인코더의 층.\n",
    "    for i in range(num_layers):\n",
    "        outputs = encoder_layer(\n",
    "            units=units,\n",
    "            d_model=d_model,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            name=\"encoder_layer_{}\".format(i),\n",
    "        )([outputs, padding_mask])\n",
    "\n",
    "    return tf.keras.Model(inputs=[inputs, padding_mask], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "id": "3e5d82ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디코더 하나의 레이어를 함수로 구현.\n",
    "# 이 하나의 레이어 안에는 세 개의 서브 레이어가 존재합니다.\n",
    "def decoder_layer(units, d_model, num_heads, dropout, name=\"decoder_layer\"):\n",
    "    inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "    enc_outputs = tf.keras.Input(shape=(None, d_model), name=\"encoder_outputs\")\n",
    "    look_ahead_mask = tf.keras.Input(shape=(1, None, None), name=\"look_ahead_mask\")\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "\n",
    "    # 첫 번째 서브 레이어 : 멀티 헤드 어텐션 수행 (셀프 어텐션)\n",
    "    attention1 = MultiHeadAttention(\n",
    "      d_model, num_heads, name=\"attention_1\")(inputs={\n",
    "          'query': inputs,\n",
    "          'key': inputs,\n",
    "          'value': inputs,\n",
    "          'mask': look_ahead_mask\n",
    "      })\n",
    "\n",
    "    # 멀티 헤드 어텐션의 결과는 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n",
    "    attention1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(attention1 + inputs)\n",
    "\n",
    "    # 두 번째 서브 레이어 : 마스크드 멀티 헤드 어텐션 수행 (인코더-디코더 어텐션)\n",
    "    attention2 = MultiHeadAttention(d_model, num_heads, name=\"attention_2\")(inputs={\n",
    "          'query': attention1,\n",
    "          'key': enc_outputs,\n",
    "          'value': enc_outputs,\n",
    "          'mask': padding_mask\n",
    "      })\n",
    "\n",
    "    # 마스크드 멀티 헤드 어텐션의 결과는\n",
    "    # Dropout과 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n",
    "    attention2 = tf.keras.layers.Dropout(rate=dropout)(attention2)\n",
    "    attention2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(attention2 + attention1)\n",
    "\n",
    "    # 세 번째 서브 레이어 : 2개의 완전연결층\n",
    "    outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention2)\n",
    "    outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "\n",
    "    # 완전연결층의 결과는 Dropout과 LayerNormalization 수행\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "    outputs = tf.keras.layers.LayerNormalization(epsilon=1e-6)(outputs + attention2)\n",
    "\n",
    "    return tf.keras.Model(inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],outputs=outputs,name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "id": "31a8406f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder(vocab_size,\n",
    "            num_layers,\n",
    "            units,\n",
    "            d_model,\n",
    "            num_heads,\n",
    "            dropout,\n",
    "            name='decoder'):\n",
    "    inputs = tf.keras.Input(shape=(None,), name='inputs')\n",
    "    enc_outputs = tf.keras.Input(shape=(None, d_model), name='encoder_outputs')\n",
    "    look_ahead_mask = tf.keras.Input(shape=(1, None, None), name='look_ahead_mask')\n",
    "\n",
    "    # 패딩 마스크\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "\n",
    "    # 임베딩 레이어\n",
    "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "\n",
    "    # 포지셔널 인코딩\n",
    "    embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "\n",
    "    # Dropout이라는 훈련을 돕는 테크닉을 수행\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "    for i in range(num_layers):\n",
    "        outputs = decoder_layer(\n",
    "            units=units,\n",
    "            d_model=d_model,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            name='decoder_layer_{}'.format(i),\n",
    "        )(inputs=[outputs, enc_outputs, look_ahead_mask, padding_mask])\n",
    "\n",
    "    return tf.keras.Model(inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],outputs=outputs,name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a66d86e",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "id": "f2df51d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer(vocab_size,\n",
    "                num_layers,\n",
    "                units,\n",
    "                d_model,\n",
    "                num_heads,\n",
    "                dropout,\n",
    "                name=\"transformer\"):\n",
    "    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "    dec_inputs = tf.keras.Input(shape=(None,), name=\"dec_inputs\")\n",
    "\n",
    "    # 인코더에서 패딩을 위한 마스크\n",
    "    enc_padding_mask = tf.keras.layers.Lambda(\n",
    "                              create_padding_mask, output_shape=(1, 1, None),\n",
    "                              name='enc_padding_mask')(inputs)\n",
    "\n",
    "    # 디코더에서 미래의 토큰을 마스크 하기 위해서 사용합니다.\n",
    "    # 내부적으로 패딩 마스크도 포함되어져 있습니다.\n",
    "    look_ahead_mask = tf.keras.layers.Lambda(\n",
    "                              create_look_ahead_mask,\n",
    "                              output_shape=(1, None, None),\n",
    "                              name='look_ahead_mask')(dec_inputs)\n",
    "\n",
    "    # 두 번째 어텐션 블록에서 인코더의 벡터들을 마스킹\n",
    "    # 디코더에서 패딩을 위한 마스크\n",
    "    dec_padding_mask = tf.keras.layers.Lambda(\n",
    "                              create_padding_mask, output_shape=(1, 1, None),\n",
    "                              name='dec_padding_mask')(inputs)\n",
    "\n",
    "    # 인코더\n",
    "    enc_outputs = encoder(\n",
    "                  vocab_size=vocab_size,\n",
    "                  num_layers=num_layers,\n",
    "                  units=units,\n",
    "                  d_model=d_model,\n",
    "                  num_heads=num_heads,\n",
    "                  dropout=dropout,\n",
    "                )(inputs=[inputs, enc_padding_mask])\n",
    "\n",
    "    # 디코더\n",
    "    dec_outputs = decoder(\n",
    "                  vocab_size=vocab_size,\n",
    "                  num_layers=num_layers,\n",
    "                  units=units,\n",
    "                  d_model=d_model,\n",
    "                  num_heads=num_heads,\n",
    "                  dropout=dropout,\n",
    "                )(inputs=[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask])\n",
    "\n",
    "    # 완전연결층\n",
    "    outputs = tf.keras.layers.Dense(units=vocab_size, name=\"outputs\")(dec_outputs)\n",
    "    return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "id": "74dbb0fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "inputs (InputLayer)             [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dec_inputs (InputLayer)         [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "enc_padding_mask (Lambda)       (None, 1, 1, None)   0           inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "encoder (Functional)            (None, None, 256)    2881536     inputs[0][0]                     \n",
      "                                                                 enc_padding_mask[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "look_ahead_mask (Lambda)        (None, 1, None, None 0           dec_inputs[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dec_padding_mask (Lambda)       (None, 1, 1, None)   0           inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "decoder (Functional)            (None, None, 256)    3145728     dec_inputs[0][0]                 \n",
      "                                                                 encoder[0][0]                    \n",
      "                                                                 look_ahead_mask[0][0]            \n",
      "                                                                 dec_padding_mask[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "outputs (Dense)                 (None, None, 8166)   2098662     decoder[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 8,125,926\n",
      "Trainable params: 8,125,926\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# 하이퍼파라미터\n",
    "NUM_LAYERS = 2 # 인코더와 디코더의 층의 개수\n",
    "D_MODEL = 256 # 인코더와 디코더 내부의 입, 출력의 고정 차원\n",
    "NUM_HEADS = 8 # 멀티 헤드 어텐션에서의 헤드 수 \n",
    "UNITS = 512 # 피드 포워드 신경망의 은닉층의 크기\n",
    "DROPOUT = 0.1 # 드롭아웃의 비율\n",
    "\n",
    "model = transformer(\n",
    "            vocab_size=VOCAB_SIZE,\n",
    "            num_layers=NUM_LAYERS,\n",
    "            units=UNITS,\n",
    "            d_model=D_MODEL,\n",
    "            num_heads=NUM_HEADS,\n",
    "            dropout=DROPOUT)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "id": "20f2f6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(y_true, y_pred):\n",
    "    y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')(y_true, y_pred)\n",
    "    mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
    "    loss = tf.multiply(loss, mask)\n",
    "\n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "id": "886ffb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps**-1.5)\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "id": "c7e02c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(D_MODEL)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "    return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "id": "509c8fea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "185/185 [==============================] - 19s 67ms/step - loss: 1.1560 - accuracy: 0.0258\n",
      "Epoch 2/30\n",
      "185/185 [==============================] - 12s 67ms/step - loss: 0.9352 - accuracy: 0.0395\n",
      "Epoch 3/30\n",
      "185/185 [==============================] - 12s 67ms/step - loss: 0.7937 - accuracy: 0.0407\n",
      "Epoch 4/30\n",
      "185/185 [==============================] - 13s 68ms/step - loss: 0.7301 - accuracy: 0.0439\n",
      "Epoch 5/30\n",
      "185/185 [==============================] - 13s 68ms/step - loss: 0.6822 - accuracy: 0.0465\n",
      "Epoch 6/30\n",
      "185/185 [==============================] - 13s 68ms/step - loss: 0.6327 - accuracy: 0.0502\n",
      "Epoch 7/30\n",
      "185/185 [==============================] - 13s 68ms/step - loss: 0.5821 - accuracy: 0.0546\n",
      "Epoch 8/30\n",
      "185/185 [==============================] - 13s 68ms/step - loss: 0.5295 - accuracy: 0.0601\n",
      "Epoch 9/30\n",
      "185/185 [==============================] - 13s 68ms/step - loss: 0.4749 - accuracy: 0.0655\n",
      "Epoch 10/30\n",
      "185/185 [==============================] - 13s 68ms/step - loss: 0.4190 - accuracy: 0.0707\n",
      "Epoch 11/30\n",
      "185/185 [==============================] - 13s 68ms/step - loss: 0.3637 - accuracy: 0.0762\n",
      "Epoch 12/30\n",
      "185/185 [==============================] - 13s 68ms/step - loss: 0.3107 - accuracy: 0.0826\n",
      "Epoch 13/30\n",
      "185/185 [==============================] - 13s 68ms/step - loss: 0.2596 - accuracy: 0.0896\n",
      "Epoch 14/30\n",
      "185/185 [==============================] - 13s 68ms/step - loss: 0.2123 - accuracy: 0.0971\n",
      "Epoch 15/30\n",
      "185/185 [==============================] - 13s 68ms/step - loss: 0.1694 - accuracy: 0.1042\n",
      "Epoch 16/30\n",
      "185/185 [==============================] - 13s 68ms/step - loss: 0.1323 - accuracy: 0.1111\n",
      "Epoch 17/30\n",
      "185/185 [==============================] - 13s 68ms/step - loss: 0.1031 - accuracy: 0.1166\n",
      "Epoch 18/30\n",
      "185/185 [==============================] - 13s 68ms/step - loss: 0.0811 - accuracy: 0.1210\n",
      "Epoch 19/30\n",
      "185/185 [==============================] - 13s 68ms/step - loss: 0.0663 - accuracy: 0.1237\n",
      "Epoch 20/30\n",
      "185/185 [==============================] - 13s 68ms/step - loss: 0.0565 - accuracy: 0.1257\n",
      "Epoch 21/30\n",
      "185/185 [==============================] - 13s 68ms/step - loss: 0.0500 - accuracy: 0.1269\n",
      "Epoch 22/30\n",
      "185/185 [==============================] - 13s 68ms/step - loss: 0.0461 - accuracy: 0.1279\n",
      "Epoch 23/30\n",
      "185/185 [==============================] - 13s 68ms/step - loss: 0.0408 - accuracy: 0.1290\n",
      "Epoch 24/30\n",
      "185/185 [==============================] - 13s 68ms/step - loss: 0.0356 - accuracy: 0.1303\n",
      "Epoch 25/30\n",
      "185/185 [==============================] - 13s 68ms/step - loss: 0.0307 - accuracy: 0.1315\n",
      "Epoch 26/30\n",
      "185/185 [==============================] - 13s 68ms/step - loss: 0.0265 - accuracy: 0.1326\n",
      "Epoch 27/30\n",
      "185/185 [==============================] - 13s 68ms/step - loss: 0.0246 - accuracy: 0.1330\n",
      "Epoch 28/30\n",
      "185/185 [==============================] - 13s 68ms/step - loss: 0.0221 - accuracy: 0.1335\n",
      "Epoch 29/30\n",
      "185/185 [==============================] - 13s 68ms/step - loss: 0.0197 - accuracy: 0.1343\n",
      "Epoch 30/30\n",
      "185/185 [==============================] - 13s 68ms/step - loss: 0.0182 - accuracy: 0.1347\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7aebd0b55e80>"
      ]
     },
     "execution_count": 632,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EPOCHS = 30\n",
    "model.fit(dataset, epochs=EPOCHS, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e21ed4",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "id": "ae4cc4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_inference(sentence):\n",
    "  sentence = preprocess_sentence(sentence)\n",
    "\n",
    "  # 입력된 문장을 정수 인코딩 후, 시작 토큰과 종료 토큰을 앞뒤로 추가.\n",
    "  # ex) Where have you been? → [[8331   86   30    5 1059    7 8332]]\n",
    "  sentence = tf.expand_dims(\n",
    "      START_TOKEN + tokenizer.encode(sentence) + END_TOKEN, axis=0)\n",
    "\n",
    "  # 디코더의 현재까지의 예측한 출력 시퀀스가 지속적으로 저장되는 변수.\n",
    "  # 처음에는 예측한 내용이 없음으로 시작 토큰만 별도 저장. ex) 8331\n",
    "  output_sequence = tf.expand_dims(START_TOKEN, 0)\n",
    "\n",
    "  # 디코더의 인퍼런스 단계\n",
    "  for i in range(MAX_LENGTH):\n",
    "    # 디코더는 최대 MAX_LENGTH의 길이만큼 다음 단어 예측을 반복합니다.\n",
    "    predictions = model(inputs=[sentence, output_sequence], training=False)\n",
    "    predictions = predictions[:, -1:, :]\n",
    "\n",
    "    # 현재 예측한 단어의 정수\n",
    "    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "\n",
    "    # 만약 현재 예측한 단어가 종료 토큰이라면 for문을 종료\n",
    "    if tf.equal(predicted_id, END_TOKEN[0]):\n",
    "      break\n",
    "\n",
    "    # 예측한 단어들은 지속적으로 output_sequence에 추가됩니다.\n",
    "    # 이 output_sequence는 다시 디코더의 입력이 됩니다.\n",
    "    output_sequence = tf.concat([output_sequence, predicted_id], axis=-1)\n",
    "\n",
    "  return tf.squeeze(output_sequence, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "id": "96ae1a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_generation(sentence):\n",
    "    # 입력 문장에 대해서 디코더를 동작 시켜 예측된 정수 시퀀스를 리턴받습니다.\n",
    "    prediction = decoder_inference(sentence)\n",
    "\n",
    "    # 정수 시퀀스를 다시 텍스트 시퀀스로 변환합니다.\n",
    "    predicted_sentence = tokenizer.decode(\n",
    "      [i for i in prediction if i < tokenizer.vocab_size])\n",
    "\n",
    "    print('입력 : {}'.format(sentence))\n",
    "    print('출력 : {}'.format(predicted_sentence))\n",
    "\n",
    "    return predicted_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "id": "76165111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 언제?\n",
      "출력 : 언제부터인지 모르게 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'언제부터인지 모르게 .'"
      ]
     },
     "execution_count": 638,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation('언제?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "0734f89d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 너야?\n",
      "출력 : 네\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'네'"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation('너야?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f13ebeb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 넌 어때?\n",
      "출력 : 당신도 예뻐요 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'당신도 예뻐요 .'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation('넌 어때?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a5c759",
   "metadata": {},
   "source": [
    "### Test with different HP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "id": "d2ec065b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# # 하이퍼파라미터 Epoch 30\n",
    "# NUM_LAYERS = 2 # 인코더와 디코더의 층의 개수\n",
    "# D_MODEL = 256 # 인코더와 디코더 내부의 입, 출력의 고정 차원\n",
    "# NUM_HEADS = 8 # 멀티 헤드 어텐션에서의 헤드 수 \n",
    "# UNITS = 512 # 피드 포워드 신경망의 은닉층의 크기\n",
    "# DROPOUT = 0.1 # 드롭아웃의 비율\n",
    "# '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "id": "0746308c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dense model has size=31.00 MiB\n"
     ]
    }
   ],
   "source": [
    "model_size = get_model_size(model)\n",
    "print(f\"dense model has size={model_size/MiB:.2f} MiB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "f53fee84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# # 하이퍼파라미터 Epoch 30\n",
    "# NUM_LAYERS = 6# 인코더와 디코더의 층의 개수\n",
    "# D_MODEL = 512 # 인코더와 디코더 내부의 입, 출력의 고정 차원\n",
    "# NUM_HEADS = 8 # 멀티 헤드 어텐션에서의 헤드 수 \n",
    "# UNITS = 512 # 피드 포워드 신경망의 은닉층의 크기\n",
    "# DROPOUT = 0.1 # 드롭아웃의 비율\n",
    "# '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "db66ad7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dense model has accuracy=0.11%\n",
      "dense model has size=108.11 MiB\n"
     ]
    }
   ],
   "source": [
    "print(f\"dense model has accuracy={model_accuracy:.2f}%\")\n",
    "print(f\"dense model has size={model_size/MiB:.2f} MiB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "94096ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [[8163 1229 7655 8164]]\n",
      "Output: 전한 얽해보세요   \n"
     ]
    }
   ],
   "source": [
    "output = answer(\"씨끄러워\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "88107fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [[8163 1641  147 8164]]\n",
      "Output: 저도 영화 보여주세요   \n"
     ]
    }
   ],
   "source": [
    "output = answer(\"운동하자\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "881312b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [[8163  878 7559 8164]]\n",
      "Output: 저는 도움이 되고 싶어요\n"
     ]
    }
   ],
   "source": [
    "output = answer(\"너 누구야\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "cbc196b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [[8163  238 8164]]\n",
      "Output: 가지마요   \n"
     ]
    }
   ],
   "source": [
    "output = answer('싫어')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ce1954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# # 하이퍼파라미터 Epoch 100\n",
    "# NUM_LAYERS = 6# 인코더와 디코더의 층의 개수\n",
    "# D_MODEL = 512 # 인코더와 디코더 내부의 입, 출력의 고정 차원\n",
    "# NUM_HEADS = 8 # 멀티 헤드 어텐션에서의 헤드 수 \n",
    "# UNITS = 512 # 피드 포워드 신경망의 은닉층의 크기\n",
    "# DROPOUT = 0.1 # 드롭아웃의 비율\n",
    "# '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "a99c5292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [[8163  238 8164]]\n",
      "Output: 싫어하지 말아요   \n"
     ]
    }
   ],
   "source": [
    "output = answer('싫어')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "d0a33367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [[8163 1641  147 8164]]\n",
      "Output: 오늘 강추위래요   \n"
     ]
    }
   ],
   "source": [
    "output = answer(\"운동하자\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "425d12a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [[8163 1229 7655 8164]]\n",
      "Output: 오늘은 예능이요   \n"
     ]
    }
   ],
   "source": [
    "output = answer(\"씨끄러워\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "d5e2dc7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [[8163 7903  239   38 3052 8164]]\n",
      "Output: 하루가 또 함께 있는 조별로에요   \n"
     ]
    }
   ],
   "source": [
    "output = answer('12시 ! 땡')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e0aca0",
   "metadata": {},
   "source": [
    "Summary:  \n",
    "1. 작은모델에서 답변이 부족하다\n",
    "2. 모델크기를 늘렸을때 같은 에폭에서 더 이상한 답변을 내놓는다. \n",
    "3. 모델크기를 늘렸을때에는, epochs수를 더 늘려서 성능을 올릴 수 있다. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4d6251",
   "metadata": {},
   "source": [
    "### Attention Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "id": "6b8fe38c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs InputLayer\n",
      "embedding Embedding\n",
      "tf.math.multiply TFOpLambda\n",
      "positional_encoding PositionalEncoding\n",
      "dropout Dropout\n",
      "padding_mask InputLayer\n",
      "encoder_layer_0 Functional\n",
      "encoder_layer_1 Functional\n"
     ]
    }
   ],
   "source": [
    "for layer in model.get_layer('encoder').layers:\n",
    "    print(layer.name, layer.__class__.__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "66c14206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Issue report (https://github.com/keras-team/tf-keras/issues/206)\n",
    "\n",
    "# How to convert KerasTensor to numpy array or TensorFlow EagerTensor? #206\n",
    "# -> The Keras Tensor cannot be converted to a Numpy array directly, You can please try to convert the Keras tensor to Tensor and from the Tensor you can convert to the numpy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9be982",
   "metadata": {},
   "source": [
    "Issue:\n",
    "\n",
    "1. attention weights 시각화를 위해 Keras tensor에서의 값을 numpy변환할때에는, 직접변환이 불가능하며 tensor로 먼저 변환후 numpy변환해야 한다.\n",
    "2. weights 확인시 lookup head mask 처리를 해주어야한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f3d308",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
